{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GlzaB2vPYD2"
      },
      "source": [
        "# Project :8 Working with Textual Data:  Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkXr2l00PfWp"
      },
      "source": [
        "In the realm of entertainment, understanding audience sentiment towards movies is paramount for studios and production houses aiming to gauge the reception of their cinematic offerings. The IMDB movie review sentiment classification problem presents a crucial business challenge: accurately classifying movie reviews into positive or negative sentiments. What adds complexity to this task is the inherent variability in review lengths, the diverse vocabulary of words used, and the necessity for the model to discern the intricate long-term dependencies and contextual nuances embedded within the text.\n",
        "\n",
        "  Create a first step document that lists the output of your exploratory analysis, any issues, or problems you may see with data that need follow-up, and some basic descriptive analysis that you think highlights important outcomes/findings from the data. Based on your findings, the next level of analysis will be charted out. Build a predictive model to classify the reviews into positive or negative. Perform a comparative study of several predictive models with various approaches and give your inferences accordingly.\n",
        "\n",
        "**Dataset description:**  \n",
        "\n",
        "There are 2 columns, review, and sentiment with 50000 records. ‘review’ column consists of imdb reviews. sentiment is the target variable with 2 classes (positive and negative)\n",
        "\n",
        "**Dataset: IMDB Dataset.csv**\n",
        "\n",
        "**Software Engineering aspect:**  \n",
        "\n",
        "Utilize software engineering aspects while building the model using modular programming principles to organize your code into reusable functions or classes to enhance readability, maintainability, and collaboration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQvJda9rQA7r"
      },
      "source": [
        "**Initial Guidelines:**\n",
        "\n",
        "1.\tEnsure to follow to User Id’s provided by UNext for naming file as conventions.\n",
        "2.\tCreate GitHub account and submit the GitHub link.\n",
        "3. Task 1.5 to 2.6 may require use of GPU.\n",
        "4. Learners can request for GPU based instance on demand by sending an email to `corpsupport@u-next.com`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWYanpaIPfcm"
      },
      "source": [
        "### General Instructions\n",
        "\n",
        "- The cells in the Jupyter notebook can be executed any number of times for testing the solution\n",
        "- Refrain from modifying the boilerplate code as it may lead to unexpected behavior \n",
        "- The solution is to be written between the comments `# code starts here` and `# code ends here`\n",
        "- On completing all the questions, the assessment is to be submitted on moodle for evaluation\n",
        "- Before submitting the assessment, there should be `no error` while executing the notebook. If there are any error causing code, please comment it.\n",
        "- The kernel of the Jupyter notebook is to be set as `Python 3 (ipykernel)` if not set already\n",
        "- Include imports as necessary\n",
        "- For each of the task, `Note` section will provide you hints to solve the problem.\n",
        "- Do not use `PRINT` statement inside the `Except` Block. Please use `return` statement only within the except block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mZCmn0GRS4e",
        "outputId": "cd8ba1bb-1463-44ec-bb3b-88bdfac30e1f"
      },
      "outputs": [],
      "source": [
        "#Required imports\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKtRWSHvRaQu"
      },
      "source": [
        "# Task 1: Load the dataset and perform preliminary EDA with key observations and insights- (weightage - 20 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50YSUPrCRkP_"
      },
      "source": [
        "#### T1.1: Load the IMDB Dataset using try and except blocks .           (weightage - 2 marks) (AE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note:\n",
        "- Define a function named `load_the_dataset()` that attempts to load data from a CSV file named \"IMDB Dataset.csv\".\n",
        "- Use a `try-except` block inside the function.\n",
        "- Try to read the CSV file using `pd.read_csv()`.\n",
        "- If successful, return the dataset (`df`).\n",
        "- If there's an error (e.g., file not found), return the message \"File not found. Please check the file path.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpM2bvRFN-wq"
      },
      "outputs": [],
      "source": [
        "def load_the_dataset():\n",
        "    try:\n",
        "        \n",
        "        return \n",
        "    except :\n",
        "        return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- After defining the function, call it to load the dataset and assign it to the variable `df`.\n",
        "- Print the first few rows of the dataset using `print(df.head())`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwQsjKrBScQ6",
        "outputId": "f78266fe-31fe-4073-9fcc-b14aeedb4b2b"
      },
      "outputs": [],
      "source": [
        "# store the result of the dataset\n",
        "df=load_the_dataset()\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQVFhKjVS7MD"
      },
      "source": [
        "#### T1.2: Check for distribution of target variable(percentage)(weightage - 2 marks)  (AE)             "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note:\n",
        "- Define a function `target_class(df)` to analyze the distribution of a target variable ('sentiment') within a DataFrame (df).\n",
        "- Inside the function, calculate the proportion of each unique value in the 'sentiment' column using the `value_counts` method with `normalize=True`.\n",
        "- Multiply the proportions by 100 to obtain percentages.\n",
        "- Return the calculated distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Fy0jv_hQ-NA"
      },
      "outputs": [],
      "source": [
        "def target_class(df):\n",
        "    #code starts here\n",
        "    \n",
        "    #code ends\n",
        "    return \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Call the function with `df` as argument to get `target_distribution`.\n",
        "- Print `target_distribution`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgfFuXiPTeZm",
        "outputId": "9df132f1-8489-44b2-8485-7c8007a77990"
      },
      "outputs": [],
      "source": [
        "# store the result\n",
        "target_distribution = target_class(df)\n",
        "print(target_distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uja4gf83TM-k"
      },
      "source": [
        "#### T1.3: Clean individual reviews: Remove all punctuations from words. Remove HTML tags. Remove words between square brackets. Remove all words that are not purely alphabetical characters. Convert all words to lowercase. Use error handling technique. (Weightage 5marks)(ME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note:\n",
        "- Define a function `strip_html(text)` to remove HTML tags using BeautifulSoup.\n",
        "- Define a function `remove_between_square_brackets(text)` to remove text within square brackets using regex.\n",
        "- Define a function `denoise_text(text)` to apply `strip_html()` and `remove_between_square_brackets()` to lowercased text. Handle exceptions using `try-except`.\n",
        "- Do not use `PRINT` statement inside the `Except` Block. Please use `return` statement only within the except block\n",
        "- Define a function `remove_special_characters(text, remove_digits=True)` to remove special characters using regex.\n",
        "- Define a function `remove_punctuation(text)` to remove punctuation using regex.\n",
        "- Apply `denoise_text()`, `remove_special_characters()`, and `remove_punctuation()` functions to the 'review' column in DataFrame `df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3HcukkQRdbj"
      },
      "outputs": [],
      "source": [
        "def strip_html(text):\n",
        "   \n",
        "    return \n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return \n",
        "\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "  try:\n",
        "   \n",
        "    return \n",
        "  except:\n",
        "    return\n",
        "   \n",
        "\n",
        "#Define function for removing special characters\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    \n",
        "    return \n",
        "\n",
        "#Apply function on review column\n",
        "def remove_punctuation(text):\n",
        "    # Define a regex pattern to match punctuation\n",
        "   \n",
        "    # Use the sub() function to replace punctuation with an empty string\n",
        "\n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming 'df' is your pandas DataFrame containing the IMDb dataset,\n",
        "# and 'review_column' is the name of the column containing the reviews\n",
        "df['review']=df['review'].apply(denoise_text)\n",
        "df['review']=df['review'].apply(remove_special_characters)\n",
        "df['review']=df['review'].apply(remove_punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Print the first few rows of `df` to view the cleaned data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Bkr2uoVZV_CB",
        "outputId": "d433ff1d-b6f5-4f6e-eb7d-cbc8c0e362ea"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AuaWCF5Tmvb"
      },
      "source": [
        "#### T1.4: Count the number of stopwords present. Remove all words that are known stop words.  (Use nltk)  (weightage - 2 marks)(AE)        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note:\n",
        "- Import necessary modules from NLTK for text preprocessing and download NLTK stopwords dataset if not already downloaded.\n",
        "- Define two functions: `num_stopwords(review)` and `remove_stopwords(review)`.\n",
        "- Tokenize the input review using NLTK's `word_tokenize` function.\n",
        "- In `num_stopwords(review)`, count the number of stopwords present in the tokenized review.\n",
        "- Return the count of stopwords.\n",
        "- In `remove_stopwords(review)`, remove stopwords from the tokenized review.\n",
        "- Join the cleaned tokens back into a string.\n",
        "- Return the cleaned review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5StZmlpKTixg",
        "outputId": "5b8da72a-38c9-4e22-aba9-953e3900a9e0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Download stopwords if not already downloaded\n",
        "\n",
        "\n",
        "def num_stopwords(review):\n",
        "    # Tokenize the review\n",
        "    \n",
        "    # Get English stopwords\n",
        "   \n",
        "    # Count the number of stopwords present\n",
        "    return \n",
        "\n",
        "def remove_stopwords(review):\n",
        "\n",
        "    # Tokenize the review\n",
        "    \n",
        "    # Get English stopwords\n",
        "    \n",
        "    # Remove stopwords\n",
        "    \n",
        "    # Join tokens back into a cleaned review\n",
        "    \n",
        "    return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Apply the `num_stopwords` method to each review in the 'review' column using the `apply` function.\n",
        "- Sum the total number of stopwords across all reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPHPwrAnbKxQ",
        "outputId": "998b9a71-b861-4bfe-aee4-0a9a9ae9d72f"
      },
      "outputs": [],
      "source": [
        "# Apply function to the specified column in the DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Apply the `remove_stopwords` function to the 'review' column of DataFrame `df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t54yc941Wu6g"
      },
      "outputs": [],
      "source": [
        "# remove stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Remove the 'num_stopwords' column from the DataFrame `df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpRmc_fWq4-n"
      },
      "outputs": [],
      "source": [
        "# Drop the num_stopwords column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### __Task 1.5 to 2.6 may require use of GPU.__\n",
        "#### Learners can request for GPU based instance on demand by sending an email to `corpsupport@u-next.com`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8jITjlHcGvS"
      },
      "source": [
        "## T1.5: Remove all words that have a length of 1 character. Count the number of such words removed.Perform lemmatization to reduce words to their base form. (weightage – 4 marks)   (AE) & (ME)       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note:\n",
        "- Define a function `count_short_words(review)` to count short words in a review.\n",
        "- Tokenize the review into words.\n",
        "- Count words with a length of 1.\n",
        "- Return the count of short words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmjCvvaNSggt"
      },
      "outputs": [],
      "source": [
        "def count_short_words(review):#AE\n",
        "    # Split the review into tokens\n",
        "    \n",
        "    # Count words with length 1\n",
        "    \n",
        "    return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Define a function `count_short_words_apply` to count short words in a specified column of a DataFrame.\n",
        "- Apply the `count_short_words` function to the specified column in the DataFrame and sum up the counts of short words.\n",
        "- Return the total count of short words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdN8C1nVdf3h"
      },
      "outputs": [],
      "source": [
        "def count_short_words_apply(df, column_name):#AE\n",
        "    # Apply count_short_words function to the specified column in the DataFrame\n",
        "   \n",
        "    return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Define a function called `remove_shortwords` to filter out short words from a given review.\n",
        "- Tokenize the review into individual words using `word_tokenize`.\n",
        "- Create a new list (`new`) containing only words with a length greater than 1, Join the filtered words back into a single string and return the filtered review string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv99LpPgcUvG"
      },
      "outputs": [],
      "source": [
        "def remove_shortwords(review):#ME\n",
        " \n",
        "  return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Apply the function `count_short_words_apply` to the DataFrame `df` using the column 'review'.\n",
        "- Print the total number of words with a length of 1 character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k955nVNxmK83",
        "outputId": "18209877-3b3b-44a3-9434-cbc35eb128b8"
      },
      "outputs": [],
      "source": [
        "total_short_words = count_short_words_apply(df, 'review')#AE\n",
        "print(\"Total number of words with length = 1 character:\", total_short_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Apply the `remove_shortwords` function to the 'review' column in DataFrame `df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXV2ft9tdJjW"
      },
      "outputs": [],
      "source": [
        "# Apply remove_shortwords\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Define a function `simple_lemmatizer` to perform lemmatization on text data.\n",
        "- Tokenize the input text into individual words.\n",
        "- Perform part-of-speech (POS) tagging on the words to determine their grammatical category (noun, verb, adjective, adverb).\n",
        "- Lemmatize each word based on its POS tag, using WordNet lemmatization.\n",
        "- Join the lemmatized words back into a string.\n",
        "- Apply the `simple_lemmatizer` function to each review in the 'review' column of DataFrame `df`.\n",
        "- Print the first few rows of the DataFrame using df.head()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBstV4XSeoo_",
        "outputId": "9c9f27e6-4971-478c-aa99-41ee8eeed7bc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "\n",
        "\n",
        "# Initialize the WordNet lemmatizer\n",
        "\n",
        "\n",
        "# Function to perform lemmatization on text\n",
        "def simple_lemmatizer(text):\n",
        "    # Tokenize the text into words\n",
        "   \n",
        "    # Perform POS tagging\n",
        "    \n",
        "    # Lemmatize each word based on its POS tag\n",
        "    \n",
        "    # Join the lemmatized words back into a string\n",
        "    return \n",
        "\n",
        "# Test the lemmatizer function with a sample text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3NWKihqoYIVb",
        "outputId": "1c489604-5c0d-4e45-887c-8e12ed8c8453"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XEWBTkargkv"
      },
      "source": [
        "#### T1.6: Create wordcloud for positive and negative sentiments.             (weightage - 5 marks)            (ME)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note:\n",
        "- Define a WordCloud object named `WC` with the width of 1000 pixels, height of 500 pixels, maximum words of 500, and minimum font size of 5.\n",
        "- Generate a WordCloud for positive reviews by joining all the review text where the sentiment is positive.\n",
        "- Display the generated WordCloud using matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "_WSgHlzR43UK",
        "outputId": "d9be11ba-e45a-4a1b-d272-d3099b4e6d80"
      },
      "outputs": [],
      "source": [
        "# word cloud for positive review words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Define a method `create_wordcloud` to generate a word cloud for negative review words.\n",
        "- Join all the negative review texts into a single string.\n",
        "- Create a WordCloud object with specified parameters (width, height, max_words, min_font_size).\n",
        "- Generate the word cloud using the negative review text.\n",
        "- Display the generated word cloud using Matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "eVfzOOWG6nyX",
        "outputId": "e7eed420-390d-470b-8a85-a8014a4f3bba"
      },
      "outputs": [],
      "source": [
        "# word cloud for negative review words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIcXPrLZgfX-"
      },
      "source": [
        "# Task 2: Build a Neural Network Predictive Model with Randomized Search (weightage - 30 marks)      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2wzbvMqgjDf"
      },
      "source": [
        "#### T2.1: Load the cleaned dataset and divide it into predictor and target values (X & y) (weightage – 3 marks) (AE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note:\n",
        "- Define a function `separate_data_and_target` to split a DataFrame into input features (X) and target variable (y).\n",
        "- Inside the function, extract the 'review' column as input features (X) and the 'sentiment' column as the target variable (y).\n",
        "- Return X and y as separate entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-uaARZsgtxk"
      },
      "outputs": [],
      "source": [
        "# Splitting into input features and output(target variable)\n",
        "# Separate independent features and target variable\n",
        "def separate_data_and_target(df):\n",
        "    \n",
        "    return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Assign the features to variable X and the target variable to variable y.\n",
        "- Print the first few rows of X and by using `X.head()` and `y.head()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViWYJTAlg4Rg",
        "outputId": "f7e93489-ab9d-41dc-da91-2e977fbdc03d"
      },
      "outputs": [],
      "source": [
        "X, y = separate_data_and_target(df)\n",
        "print(X.head())\n",
        "print(y.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcQu2jdy7Fsl"
      },
      "source": [
        "#### T2.2 Handling categorical features: Use TF-IDF vectorizer with max_features 5000 to convert into numerical features. Convert target variable, sentiment positive to 1 and negative to 0 (weightage - 2 marks)    (AE and ME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note:\n",
        "- Define a function named `label_encode` that converts 'positive' sentiment to 1 and other sentiments to 0.\n",
        "- Use the `map` method to apply the `label_encode` function to the 'sentiment' column of DataFrame `df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkBWOn267IuJ"
      },
      "outputs": [],
      "source": [
        "def label_encode(sentiment):\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['sentiment'] = df['sentiment'].map(label_encode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Transform text data into TF-IDF vectors.\n",
        "- Specify the maximum number of features to consider using the `max_features` parameter as 5000.\n",
        "- Use the `fit_transform` method to transform the text data into TF-IDF vectors.\n",
        "- Assign the transformed data to the variable `X_tfidf`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vERPcmZjfLnS"
      },
      "outputs": [],
      "source": [
        "#set max_features = 5000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Convert a sparse matrix to a DataFrame.\n",
        "- Use `pd.DataFrame()` to create a DataFrame from the sparse matrix `X_tfidf`.\n",
        "- Optionally, add the 'review' column to the DataFrame using `df['review'].values`.\n",
        "- Print the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8lKT03SmUIo",
        "outputId": "d848d9bf-84b2-4b67-b0e4-c5e56932c0c9"
      },
      "outputs": [],
      "source": [
        "# Convert the sparse matrix to a DataFrame\n",
        "\n",
        "# Optional: Add the 'review' column to the DataFrame if needed\n",
        "\n",
        "# Print the DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvsIdzyNhEfr"
      },
      "source": [
        "## T2.3: Split the dataset into train and test in the ratio of 80:20. (weightage – 5 marks) (ME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note:\n",
        "- Write a method using the `train_test_split` function from the `sklearn.model_selection` module to split data into training and testing sets.\n",
        "- Use the feature matrix `X_tfidf.toarray()` and target vector `y`. \n",
        "- Set the test size to 20% and use a random state of 42 for reproducibility. The method should return `X_train`, `X_test`, `y_train`, and `y_test` arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZALLK9TYQEa"
      },
      "outputs": [],
      "source": [
        "# split into training and testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBlNE53AhoLA"
      },
      "source": [
        "#### T2.4: Train a Sequence classifier using standard Machine learning algorithm(Logistic Regression)  to classify  documents as either positive or negative . (weightage - 5 marks) (ME)\n",
        "\n",
        "**Model versioning:**\n",
        "\n",
        "- Save the model as ‘first_model’ to a version control system GitHub using git commands for collaboration, tracking changes, and ensuring transparency in model development."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Refer to the Github document from Lumen to create the repository and steps to commit \n",
        "#### Add your Github repository link below "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note:\n",
        "- Define a logistic regression model for classification (`lr`) using the sklearn library.\n",
        "- Set regularization penalty as L2, maximum iterations as 500, regularization strength as 1, and random state as 42.\n",
        "- Fit the logistic regression model (`lr_classifier`) using training data (`X_train` and `y_train`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM3otGRUYQKW"
      },
      "outputs": [],
      "source": [
        "#training the model\n",
        "\n",
        "#Fitting the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxT5f3tSiKjf"
      },
      "source": [
        "#### T2.5: Train Multilayer Perceptron (MLP) models to classify documents as either positive or negative. (weightage - 10 marks) (ME)\n",
        "\n",
        "**Model versioning**\n",
        "-Save the model as ‘second_model’ to a version control system GitHub using git commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note:\n",
        "- Import the Sequential and Dense modules from keras.models.\n",
        "- Define a Sequential model using `Sequential()`.\n",
        "- Add a dense layer with 16 units and ReLU activation using `model_mlp.add(Dense(...))`.\n",
        "- Add another dense layer with 8 units and ReLU activation.\n",
        "- Add a dense layer with 1 unit and sigmoid activation.\n",
        "- Compile the model with 'rmsprop' optimizer and binary crossentropy loss using `model_mlp.compile()`.\n",
        "- Train the model using `fit()`, specifying input data (`X_train`) and labels (`y_train`), batch size (10), and number of epochs (15)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK-mi7a6i2p-",
        "outputId": "3f72e9ea-8252-448b-d84e-2762f249efcd"
      },
      "outputs": [],
      "source": [
        "#training the model\n",
        "\n",
        "#Fitting the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_igs5c35jznV"
      },
      "source": [
        "#### T2.6: Train a CNN with Embedding layer to classify documents as either positive or negative.  (weightage-5 marks)(ME)\n",
        "\n",
        "**Model versioning**\n",
        "-  Save the model as ‘third_model’ to a version control system GitHub using git commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note:\n",
        "- Create a CNN model for text classification.\n",
        "- Import required modules from Keras and TensorFlow.\n",
        "- Set the maximum number of words and sequence length.\n",
        "- Use Tokenizer to tokenize and convert text to integers.\n",
        "- Pad sequences to ensure they're all the same length.\n",
        "- Split the data into training and testing sets.\n",
        "    - Define the CNN model's architecture:\n",
        "        - Include an embedding layer for word embeddings.\n",
        "        - Add a Conv1D layer with 128 filters and a kernel size of 5.\n",
        "        - Include a GlobalMaxPooling1D layer to reduce dimensionality.\n",
        "        - Add two Dense layers with 64 and 1 units respectively.\n",
        "- Compile the model using the Adam optimizer and binary crossentropy loss.\n",
        "- Train the model for 10 epochs with a batch size of 32 and validate the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mASSZLvMaauH",
        "outputId": "deedd4e3-9ee2-455a-b6b7-55f17804a8b9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "\n",
        "# Split the Data\n",
        "\n",
        "\n",
        "# Build the CNN Model\n",
        "\n",
        "\n",
        "# Compile the Model\n",
        "\n",
        "# Train the Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLg5N2K3jBpx"
      },
      "source": [
        "# Task 3: Evaluate the performance of the model using the right evaluation metrics.                                                                         (weightage - 25 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJs_LaL9jGFQ"
      },
      "source": [
        "#### T3.1 Bring the models from a GitHub using git commands and evaluate the model (weightage - 2marks) (ME)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hUpPhaEjRDm"
      },
      "outputs": [],
      "source": [
        "#training the model\n",
        "\n",
        "#Fitting the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S8V89NQjRrZ"
      },
      "source": [
        "#### T3.2 Evaluate the Logistic Regression model with evaluation metrics accuracy and precision using sklearn library. (weightage-5 marks) (AE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note:\n",
        "\n",
        "- __Function Definition:__ Define a function named evaluate_classification taking y_true (true labels) and X_test (test data).\n",
        "- __Prediction:__ Predict labels using a logistic regression classifier (lr_classifier) on test data (X_test) and save the result as y_pred.\n",
        "- __Evaluation Metrics:__\n",
        "    * Calculate accuracy using `accuracy_score` function.\n",
        "    * Calculate precision using `precision_score` function.\n",
        "    * Calculate recall using `recall_score` function.\n",
        "    * Calculate F1 score using `f1_score` function.\n",
        "- __Storage:__ Store all metrics in a dictionary named metrics.\n",
        "- __Return:__ Return the dictionary containing evaluation metrics.\n",
        " #### Ranges\n",
        "* Accuracy : 0.45 - 1 (2M)\n",
        "* Precision: 0.45-1 (1M) \n",
        "* Recall: 0.55-1 (1M)\n",
        "* F1 Score: 0.5 -1(1M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os63a5fyjYAs"
      },
      "outputs": [],
      "source": [
        "#Predicting the model\n",
        "def evaluate_classification(lr_classifier,y_true, X_test):\n",
        "    accuracy,precision,recall,f1 = 0.0,0.0,0.0,0.0\n",
        "\n",
        "    return accuracy,precision,recall,f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Call the function with appropriate test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZgmbu4ljdCT",
        "outputId": "7f6e23cc-4160-44db-cae7-d7e7cb5c18e2"
      },
      "outputs": [],
      "source": [
        "# call evaluate_classification\n",
        "evaluate_classification(lr_classifier,y_test, X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXqrmURrjn4T"
      },
      "source": [
        "#### T3.3 Using Lime/SHAP libraries, explain the prediction of your model and give inferences. (weightage-5 marks) (ME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vnqGUmpjwy5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uPQ9uZnjxFt"
      },
      "source": [
        "#### T3.4 For the trained MLP model used, specify the accuracy score, loss value, epochs and activation function used at the output layer of the model (weightage-8 marks)(AE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Added model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "model_mlp = Sequential()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note:\n",
        "\n",
        "Define a method named `evaluate_mlp` to assess the performance of a __Multi-layer Perceptron (MLP) model__.\n",
        "Inside the method:\n",
        "\n",
        "- Utilize the model's evaluate function to compute the loss and accuracy using the test data (X_test and y_test).\n",
        "- Determine the number of epochs by calculating the length of the loss history.\n",
        "- Extract the name of the output activation function used in the last layer of the model.\n",
        "- Return the computed loss, accuracy and the name of the output activation function.\n",
        "\n",
        "Remember to:\n",
        "- Input the trained MLP model, test data (X_test and y_test), and training history.\n",
        "- Use the method as follows: loss, accuracy, output_activation_function = evaluate_mlp(model, X_test, y_test, history)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x45_PcQGj26T"
      },
      "outputs": [],
      "source": [
        "#Evaluate the Model\n",
        "def evaluate_mlp(model,X_test,y_test,history):\n",
        "    accuracy,loss ,output_activation_function = 0.0,0.0,None\n",
        "\n",
        "    return accuracy,loss ,output_activation_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Use method evaluate_mlp to assess the MLP model's performance, then print accuracy, loss, epochs, and output activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-VGfx3lklFP",
        "outputId": "798229b6-c47f-4ac3-d73e-66a74930b049"
      },
      "outputs": [],
      "source": [
        "# Print loss, accuracy,output_activation_function \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jAlgnNHkxYi"
      },
      "source": [
        "#### T3.5 For the Trained CNN with Embedding layer, Specify the accuracy score, loss value, epochs used .(weightage-5marks) (ME)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### CNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note:\n",
        " \n",
        "__Tokenize and pad sequences:__\n",
        "- Define the maximum number of words as 10,000 and the maximum length of sequences as 100.\n",
        "- Initialize a tokenizer.\n",
        "- Teach the tokenizer about the data with **`fit_on_texts(X)`**.\n",
        "- Convert the texts to sequences using **`texts_to_sequences(X)`**.\n",
        "- Pad the sequences to ensure they're all the same length using **`pad_sequences()`**.\n",
        " \n",
        "__Split the Data:__\n",
        "- Divide the data into training and testing sets with 80% for training and 20% for testing.\n",
        "- Utilize **`train_test_split()`** with the defined parameters.\n",
        " \n",
        "__Build the CNN Model:__\n",
        "- Set the embedding dimension as 50 and the vocabulary size as the maximum words.\n",
        "- Create a Sequential model.\n",
        "- Add an Embedding layer with the specified parameters.\n",
        "- Include a 1D Convolutional layer with 128 filters, a kernel size of 5, and ReLU activation.\n",
        "- Apply GlobalMaxPooling1D to reduce the dimensionality.\n",
        "- Integrate two Dense layers with 64 and 1 neuron(s), respectively, using ReLU and sigmoid activations.\n",
        " \n",
        "__Compile the Model:__\n",
        "- Compile the model with the Adam optimizer and binary cross-entropy loss.\n",
        "- Specify **'accuracy'** as the metric for evaluation.\n",
        " \n",
        "__Train the Model:__\n",
        "- Train the model on the training data for 10 epochs with a batch size of 32.\n",
        "- Validate the model with 10% of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "\n",
        "\n",
        "# Split the Data\n",
        "\n",
        "\n",
        "# Build the CNN Model\n",
        "\n",
        "\n",
        "# Compile the Model\n",
        "\n",
        "\n",
        "# Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note:\n",
        "Create a method named `evaluate_cnn` to assess the performance of a __Convolutional Neural Network (CNN) model.__\n",
        "- Within the method:\n",
        "    - Utilize the model's evaluate function to calculate the loss and accuracy using the test data (X_test and y_test).\n",
        "    - Determine the number of epochs by extracting the length of the loss history from the training history.\n",
        "    - Return the computed loss, accuracy, and number of epochs.\n",
        "\n",
        "Remember to:\n",
        "- Provide the trained CNN model, test data (X_test and y_test), and training history as input parameters.\n",
        "- Utilize the method like this: loss, accuracy, epochs = evaluate_cnn(model, X_test, y_test, history)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUMVjwbck81v"
      },
      "outputs": [],
      "source": [
        "#Evaluate the Model\n",
        "def evaluate_cnn(model,X_test,y_test,history):\n",
        "    loss, accuracy,epochs = 0.0,0.0,0.0\n",
        "    \n",
        "    return loss, accuracy,epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Invoke the evaluate_mlp method with arguments (model_cnn, X_test, y_test, history_cnn).\n",
        "- Print the accuracy score, loss value and the number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-5NWRqQmFtb",
        "outputId": "52daef53-30be-4900-c907-50a3785b27f0"
      },
      "outputs": [],
      "source": [
        "# Print accuracy,loss,epochs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcH6vRlvmSUc"
      },
      "source": [
        "#### T3.6 Implement the unit test case and deploy a model using Flask / Streamlit. (weightage-10 marks)(ME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Note:\n",
        "\n",
        "- Import the necessary libraries: __keras.models__ for Sequential model and __keras.layers__ for Dense layers.\n",
        "- Create a new Sequential model named __keras_model.__\n",
        "- Add layers to the Sequential model and define their configurations (units, activation functions, input dimensions).\n",
        "- Set the weights of the layers in the new model to be the same as the weights of an existing model (model).\n",
        "- Save the Keras model to an HDF5 file named _'final_model.h5'.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YAKslcEsgnc",
        "outputId": "dfd79646-df8c-450b-9fbe-9248d81e9c83"
      },
      "outputs": [],
      "source": [
        "# Create a new Sequential model\n",
        "\n",
        "\n",
        "# Add layers to the Sequential model and set the weights\n",
        "\n",
        "\n",
        "# Set the weights of the layers in the new model\n",
        "\n",
        "\n",
        "# Save the Keras model to an HDF5 file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Import the necessary module json for working with JSON data.\n",
        "- Convert the tokenizer's configuration to JSON format using the __to_json()__ method.\n",
        "- Open a file named __'imdb_tokenizer.json'__ in write mode and encode it in UTF-8.\n",
        "- Write the JSON data into the file using __json.dumps()__ function, ensuring non-ASCII characters are handled properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save tokenizer configuration to JSON file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 4: Summarize the findings of the analysis and draw conclusions with PPT / PDF.                                                                                   (weightage - 15 marks) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Final Submission guidelines:** \n",
        "1.\tDownload the Jupyter notebook in the format of html. \n",
        "2.\tUpload it in the lumen (UNext LMS)\n",
        "3.\tTake a screenshot of T3.6(Deployment) and upload it in the lumen. (UNext LMS)\n",
        "4.\tSummarized PPT/ PDF prepared in Task 4 to be uploaded in the lumen. (UNext LMS)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
