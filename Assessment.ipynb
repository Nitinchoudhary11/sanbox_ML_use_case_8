{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GlzaB2vPYD2"
   },
   "source": [
    "# Project :8 Working with Textual Data:  Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkXr2l00PfWp"
   },
   "source": [
    "In the realm of entertainment, understanding audience sentiment towards movies is paramount for studios and production houses aiming to gauge the reception of their cinematic offerings. The IMDB movie review sentiment classification problem presents a crucial business challenge: accurately classifying movie reviews into positive or negative sentiments. What adds complexity to this task is the inherent variability in review lengths, the diverse vocabulary of words used, and the necessity for the model to discern the intricate long-term dependencies and contextual nuances embedded within the text.\n",
    "\n",
    "  Create a first step document that lists the output of your exploratory analysis, any issues, or problems you may see with data that need follow-up, and some basic descriptive analysis that you think highlights important outcomes/findings from the data. Based on your findings, the next level of analysis will be charted out. Build a predictive model to classify the reviews into positive or negative. Perform a comparative study of several predictive models with various approaches and give your inferences accordingly.\n",
    "\n",
    "**Dataset description:**  \n",
    "\n",
    "There are 2 columns, review, and sentiment with 50000 records. ‘review’ column consists of imdb reviews. sentiment is the target variable with 2 classes (positive and negative)\n",
    "\n",
    "**Dataset: IMDB Dataset.csv**\n",
    "\n",
    "**Software Engineering aspect:**  \n",
    "\n",
    "Utilize software engineering aspects while building the model using modular programming principles to organize your code into reusable functions or classes to enhance readability, maintainability, and collaboration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQvJda9rQA7r"
   },
   "source": [
    "**Initial Guidelines:**\n",
    "\n",
    "1.\tEnsure to follow to User Id’s provided by UNext for naming file as conventions.\n",
    "2.\tCreate GitHub account and submit the GitHub link.\n",
    "3. Task 1.5 to 2.6 may require use of GPU.\n",
    "4. Learners can request for GPU based instance on demand by sending an email to `corpsupport@u-next.com`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWYanpaIPfcm"
   },
   "source": [
    "### General Instructions\n",
    "\n",
    "- The cells in the Jupyter notebook can be executed any number of times for testing the solution\n",
    "- Refrain from modifying the boilerplate code as it may lead to unexpected behavior \n",
    "- The solution is to be written between the comments `# code starts here` and `# code ends here`\n",
    "- On completing all the questions, the assessment is to be submitted on moodle for evaluation\n",
    "- Before submitting the assessment, there should be `no error` while executing the notebook. If there are any error causing code, please comment it.\n",
    "- The kernel of the Jupyter notebook is to be set as `Python 3 (ipykernel)` if not set already\n",
    "- Include imports as necessary\n",
    "- For each of the task, `Note` section will provide you hints to solve the problem.\n",
    "- Do not use `PRINT` statement inside the `Except` Block. Please use `return` statement only within the except block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mZCmn0GRS4e",
    "outputId": "cd8ba1bb-1463-44ec-bb3b-88bdfac30e1f"
   },
   "outputs": [],
   "source": [
    "#Required imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKtRWSHvRaQu"
   },
   "source": [
    "# Task 1: Load the dataset and perform preliminary EDA with key observations and insights- (weightage - 20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50YSUPrCRkP_"
   },
   "source": [
    "#### T1.1: Load the IMDB Dataset using try and except blocks .           (weightage - 2 marks) (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- Define a function named `load_the_dataset()` that attempts to load data from a CSV file named \"IMDB Dataset.csv\".\n",
    "- Use a `try-except` block inside the function.\n",
    "- Try to read the CSV file using `pd.read_csv()`.\n",
    "- If successful, return the dataset (`df`).\n",
    "- If there's an error (e.g., file not found), return the message \"File not found. Please check the file path.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vpM2bvRFN-wq"
   },
   "outputs": [],
   "source": [
    "def load_the_dataset():\n",
    "    try:\n",
    "        df=pd.read_csv(\"IMDB_Dataset.csv\")\n",
    "        return df\n",
    "    except :\n",
    "        return \"File not found. Please check the file path\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After defining the function, call it to load the dataset and assign it to the variable `df`.\n",
    "- Print the first few rows of the dataset using `print(df.head())`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwQsjKrBScQ6",
    "outputId": "f78266fe-31fe-4073-9fcc-b14aeedb4b2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "# store the result of the dataset\n",
    "df=load_the_dataset()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQVFhKjVS7MD"
   },
   "source": [
    "#### T1.2: Check for distribution of target variable(percentage)(weightage - 2 marks)  (AE)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- Define a function `target_class(df)` to analyze the distribution of a target variable ('sentiment') within a DataFrame (df).\n",
    "- Inside the function, calculate the proportion of each unique value in the 'sentiment' column using the `value_counts` method with `normalize=True`.\n",
    "- Multiply the proportions by 100 to obtain percentages.\n",
    "- Return the calculated distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_Fy0jv_hQ-NA"
   },
   "outputs": [],
   "source": [
    "def target_class(df):\n",
    "    #code starts here\n",
    "    distribution=df['sentiment'].value_counts(normalize=True)*100\n",
    "    #code ends\n",
    "    return distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Call the function with `df` as argument to get `target_distribution`.\n",
    "- Print `target_distribution`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgfFuXiPTeZm",
    "outputId": "9df132f1-8489-44b2-8485-7c8007a77990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive    50.0\n",
      "negative    50.0\n",
      "Name: sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# store the result\n",
    "target_distribution = target_class(df)\n",
    "print(target_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uja4gf83TM-k"
   },
   "source": [
    "#### T1.3: Clean individual reviews: Remove all punctuations from words. Remove HTML tags. Remove words between square brackets. Remove all words that are not purely alphabetical characters. Convert all words to lowercase. Use error handling technique. (Weightage 5marks)(ME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- Define a function `strip_html(text)` to remove HTML tags using BeautifulSoup.\n",
    "- Define a function `remove_between_square_brackets(text)` to remove text within square brackets using regex.\n",
    "- Define a function `denoise_text(text)` to apply `strip_html()` and `remove_between_square_brackets()` to lowercased text. Handle exceptions using `try-except`.\n",
    "- Do not use `PRINT` statement inside the `Except` Block. Please use `return` statement only within the except block\n",
    "- Define a function `remove_special_characters(text, remove_digits=True)` to remove special characters using regex.\n",
    "- Define a function `remove_punctuation(text)` to remove punctuation using regex.\n",
    "- Apply `denoise_text()`, `remove_special_characters()`, and `remove_punctuation()` functions to the 'review' column in DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "l3HcukkQRdbj"
   },
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup=BeautifulSoup(text,\"html.parser\")\n",
    "   \n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'\\[.*?\\]','',text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "  try:\n",
    "    text=strip_html(text)\n",
    "    text=remove_between_square_brackets(text)\n",
    "   \n",
    "    return text.lower()\n",
    "  except:\n",
    "    return \"wrong\"\n",
    "   \n",
    "\n",
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-Z\\s]' if remove_digits else r'[^a-zA-Z0-9\\s]'\n",
    "    return re.sub(pattern,'',text)\n",
    "\n",
    "#Apply function on review column\n",
    "def remove_punctuation(text):\n",
    "    # Define a regex pattern to match punctuation\n",
    "    pattern=r'[^\\W\\S]'\n",
    "   \n",
    "    # Use the sub() function to replace punctuation with an empty string\n",
    "\n",
    "    return re.sub(pattern,\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df' is your pandas DataFrame containing the IMDb dataset,\n",
    "# and 'review_column' is the name of the column containing the reviews\n",
    "df['review']=df['review'].apply(denoise_text)\n",
    "df['review']=df['review'].apply(remove_special_characters)\n",
    "df['review']=df['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print the first few rows of `df` to view the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Bkr2uoVZV_CB",
    "outputId": "d433ff1d-b6f5-4f6e-eb7d-cbc8c0e362ea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres a family where a little boy j...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love in the time of money is a ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production the filming tech...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically theres a family where a little boy j...  negative\n",
       "4  petter matteis love in the time of money is a ...  positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AuaWCF5Tmvb"
   },
   "source": [
    "#### T1.4: Count the number of stopwords present. Remove all words that are known stop words.  (Use nltk)  (weightage - 2 marks)(AE)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- Import necessary modules from NLTK for text preprocessing and download NLTK stopwords dataset if not already downloaded.\n",
    "- Define two functions: `num_stopwords(review)` and `remove_stopwords(review)`.\n",
    "- Tokenize the input review using NLTK's `word_tokenize` function.\n",
    "- In `num_stopwords(review)`, count the number of stopwords present in the tokenized review.\n",
    "- Return the count of stopwords.\n",
    "- In `remove_stopwords(review)`, remove stopwords from the tokenized review.\n",
    "- Join the cleaned tokens back into a string.\n",
    "- Return the cleaned review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5StZmlpKTixg",
    "outputId": "5b8da72a-38c9-4e22-aba9-953e3900a9e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/labuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/labuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "def num_stopwords(review):\n",
    "    # Tokenize the review\n",
    "    tokens=word_tokenize(review)\n",
    "    # Get English stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    # Count the number of stopwords present\n",
    "    stopwords_count=sum(1 for word in tokens if word.lower() in stop_words)\n",
    "    return stopwords_count\n",
    "\n",
    "def remove_stopwords(review):\n",
    "\n",
    "    # Tokenize the review\n",
    "    tokens=word_tokenize(review)\n",
    "    # Get English stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    # Remove stopwords\n",
    "    cleaned_tokens=[word for word in tokens if word.lower() if word.lower() not in stop_words]\n",
    "    # Join tokens back into a cleaned review\n",
    "    cleaned_review=' '.join(cleaned_tokens)\n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply the `num_stopwords` method to each review in the 'review' column using the `apply` function.\n",
    "- Sum the total number of stopwords across all reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bPHPwrAnbKxQ",
    "outputId": "998b9a71-b861-4bfe-aee4-0a9a9ae9d72f"
   },
   "outputs": [],
   "source": [
    "# Apply function to the specified column in the DataFrame\n",
    "df['num_stopwords']=df['review'].apply(num_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply the `remove_stopwords` function to the 'review' column of DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "t54yc941Wu6g"
   },
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "df['review']=df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove the 'num_stopwords' column from the DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qpRmc_fWq4-n"
   },
   "outputs": [],
   "source": [
    "# Drop the num_stopwords column\n",
    "\n",
    "df=df.drop(columns=['num_stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewers mentioned watching oz episode yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres family little boy jake thinks...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one reviewers mentioned watching oz episode yo...  positive\n",
       "1  wonderful little production filming technique ...  positive\n",
       "2  thought wonderful way spend time hot summer we...  positive\n",
       "3  basically theres family little boy jake thinks...  negative\n",
       "4  petter matteis love time money visually stunni...  positive"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Task 1.5 to 2.6 may require use of GPU.__\n",
    "#### Learners can request for GPU based instance on demand by sending an email to `corpsupport@u-next.com`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8jITjlHcGvS"
   },
   "source": [
    "## T1.5: Remove all words that have a length of 1 character. Count the number of such words removed.Perform lemmatization to reduce words to their base form. (weightage – 4 marks)   (AE) & (ME)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- Define a function `count_short_words(review)` to count short words in a review.\n",
    "- Tokenize the review into words.\n",
    "- Count words with a length of 1.\n",
    "- Return the count of short words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HmjCvvaNSggt"
   },
   "outputs": [],
   "source": [
    "def count_short_words(review):#AE\n",
    "    # Split the review into tokens\n",
    "    tokens=word_tokenize(review)\n",
    "    \n",
    "    # Count words with length 1\n",
    "    short_words_count=sum(1 for word in tokens if len(word)==1)\n",
    "    \n",
    "    return short_words_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a function `count_short_words_apply` to count short words in a specified column of a DataFrame.\n",
    "- Apply the `count_short_words` function to the specified column in the DataFrame and sum up the counts of short words.\n",
    "- Return the total count of short words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OdN8C1nVdf3h"
   },
   "outputs": [],
   "source": [
    "def count_short_words_apply(df, column_name):#AE\n",
    "    # Apply count_short_words function to the specified column in the DataFrame\n",
    "    total_short_words=df[column_name].apply(count_short_words).sum()\n",
    "   \n",
    "    return total_short_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a function called `remove_shortwords` to filter out short words from a given review.\n",
    "- Tokenize the review into individual words using `word_tokenize`.\n",
    "- Create a new list (`new`) containing only words with a length greater than 1, Join the filtered words back into a single string and return the filtered review string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "nv99LpPgcUvG"
   },
   "outputs": [],
   "source": [
    "def remove_shortwords(review):#ME\n",
    "    tokens=word_tokenize(review)\n",
    "    filtered_tokens=[word for word in tokens if len(word)>1]\n",
    "    filtered_review=' '.join(filtered_tokens)\n",
    "    return filtered_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply the function `count_short_words_apply` to the DataFrame `df` using the column 'review'.\n",
    "- Print the total number of words with a length of 1 character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k955nVNxmK83",
    "outputId": "18209877-3b3b-44a3-9434-cbc35eb128b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words with length = 1 character: 6018\n"
     ]
    }
   ],
   "source": [
    "total_short_words = count_short_words_apply(df, 'review')#AE\n",
    "print(\"Total number of words with length = 1 character:\", total_short_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply the `remove_shortwords` function to the 'review' column in DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "kXV2ft9tdJjW"
   },
   "outputs": [],
   "source": [
    "# Apply remove_shortwords\n",
    "df['review']=df['review'].apply(remove_shortwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a function `simple_lemmatizer` to perform lemmatization on text data.\n",
    "- Tokenize the input text into individual words.\n",
    "- Perform part-of-speech (POS) tagging on the words to determine their grammatical category (noun, verb, adjective, adverb).\n",
    "- Lemmatize each word based on its POS tag, using WordNet lemmatization.\n",
    "- Join the lemmatized words back into a string.\n",
    "- Apply the `simple_lemmatizer` function to each review in the 'review' column of DataFrame `df`.\n",
    "- Print the first few rows of the DataFrame using df.head()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBstV4XSeoo_",
    "outputId": "9c9f27e6-4971-478c-aa99-41ee8eeed7bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/labuser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/labuser/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/labuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play play\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Function to perform lemmatization on text\n",
    "def simple_lemmatizer(text):\n",
    "    # Tokenize the text into words\n",
    "    tokens=word_tokenize(text)\n",
    "   \n",
    "    # Perform POS tagging\n",
    "    tagged_tokens=pos_tag(tokens)\n",
    "    \n",
    "    # Lemmatize each word based on its POS tag\n",
    "    lemmatized_words=[]\n",
    "    for token,tag in tagged_tokens:\n",
    "        if tag.startswith('J'):\n",
    "            pos=wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            pos=wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            pos=wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            pos=wordnet.ADV\n",
    "        else:\n",
    "            pos=wordnet.NOUN\n",
    "        lemmatized_word=lemmatizer.lemmatize(token,pos)\n",
    "        lemmatized_words.append(lemmatized_word)\n",
    "    lemmatized_text=' '.join(lemmatized_words)\n",
    "    # Join the lemmatized words back into a string\n",
    "    return lemmatized_text\n",
    "\n",
    "# Test the lemmatizer function with a sample text\n",
    "print(simple_lemmatizer('playing played'))\n",
    "df['review']=df['review'].apply(simple_lemmatizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3NWKihqoYIVb",
    "outputId": "1c489604-5c0d-4e45-887c-8e12ed8c8453",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewer mention watch oz episode youll ho...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production film technique una...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>think wonderful way spend time hot summer week...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one reviewer mention watch oz episode youll ho...  positive\n",
       "1  wonderful little production film technique una...  positive\n",
       "2  think wonderful way spend time hot summer week...  positive\n",
       "3  basically there family little boy jake think t...  negative\n",
       "4  petter matteis love time money visually stunni...  positive"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XEWBTkargkv"
   },
   "source": [
    "#### T1.6: Create wordcloud for positive and negative sentiments.             (weightage - 5 marks)            (ME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- Define a WordCloud object named `WC` with the width of 1000 pixels, height of 500 pixels, maximum words of 500, and minimum font size of 5.\n",
    "- Generate a WordCloud for positive reviews by joining all the review text where the sentiment is positive.\n",
    "- Display the generated WordCloud using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "_WSgHlzR43UK",
    "outputId": "d9be11ba-e45a-4a1b-d272-d3099b4e6d80"
   },
   "outputs": [],
   "source": [
    "# word cloud for positive review words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a method `create_wordcloud` to generate a word cloud for negative review words.\n",
    "- Join all the negative review texts into a single string.\n",
    "- Create a WordCloud object with specified parameters (width, height, max_words, min_font_size).\n",
    "- Generate the word cloud using the negative review text.\n",
    "- Display the generated word cloud using Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "id": "eVfzOOWG6nyX",
    "outputId": "e7eed420-390d-470b-8a85-a8014a4f3bba"
   },
   "outputs": [],
   "source": [
    "# word cloud for negative review words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIcXPrLZgfX-"
   },
   "source": [
    "# Task 2: Build a Neural Network Predictive Model with Randomized Search (weightage - 30 marks)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2wzbvMqgjDf"
   },
   "source": [
    "#### T2.1: Load the cleaned dataset and divide it into predictor and target values (X & y) (weightage – 3 marks) (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- Define a function `separate_data_and_target` to split a DataFrame into input features (X) and target variable (y).\n",
    "- Inside the function, extract the 'review' column as input features (X) and the 'sentiment' column as the target variable (y).\n",
    "- Return X and y as separate entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "v-uaARZsgtxk"
   },
   "outputs": [],
   "source": [
    "# Splitting into input features and output(target variable)\n",
    "# Separate independent features and target variable\n",
    "def separate_data_and_target(df):\n",
    "    # Extract the 'review' column as input features (X)\n",
    "    X = df['review']\n",
    "    # Extract the 'sentiment' column as the target variable (y)\n",
    "    y = df['sentiment']\n",
    "    # Return X and y\n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign the features to variable X and the target variable to variable y.\n",
    "- Print the first few rows of X and by using `X.head()` and `y.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ViWYJTAlg4Rg",
    "outputId": "f7e93489-ab9d-41dc-da91-2e977fbdc03d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    one reviewer mention watch oz episode youll ho...\n",
      "1    wonderful little production film technique una...\n",
      "2    think wonderful way spend time hot summer week...\n",
      "3    basically there family little boy jake think t...\n",
      "4    petter matteis love time money visually stunni...\n",
      "Name: review, dtype: object\n",
      "0    positive\n",
      "1    positive\n",
      "2    positive\n",
      "3    negative\n",
      "4    positive\n",
      "Name: sentiment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X, y = separate_data_and_target(df)\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcQu2jdy7Fsl"
   },
   "source": [
    "#### T2.2 Handling categorical features: Use TF-IDF vectorizer with max_features 5000 to convert into numerical features. Convert target variable, sentiment positive to 1 and negative to 0 (weightage - 2 marks)    (AE and ME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- Define a function named `label_encode` that converts 'positive' sentiment to 1 and other sentiments to 0.\n",
    "- Use the `map` method to apply the `label_encode` function to the 'sentiment' column of DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fkBWOn267IuJ"
   },
   "outputs": [],
   "source": [
    "def label_encode(sentiment):\n",
    "    if sentiment == 'positive':\n",
    "        p=1\n",
    "    else:\n",
    "        p=0\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].map(label_encode)\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transform text data into TF-IDF vectors.\n",
    "- Specify the maximum number of features to consider using the `max_features` parameter as 5000.\n",
    "- Use the `fit_transform` method to transform the text data into TF-IDF vectors.\n",
    "- Assign the transformed data to the variable `X_tfidf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "vERPcmZjfLnS"
   },
   "outputs": [],
   "source": [
    "#set max_features = 5000\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Step 1: Set the maximum number of features for TF-IDF\n",
    "max_features = 5000\n",
    "\n",
    "# Step 2: Initialize the TfidfVectorizer with max_features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "\n",
    "# Step 3: Transform the text data into TF-IDF vectors\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['review'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert a sparse matrix to a DataFrame.\n",
    "- Use `pd.DataFrame()` to create a DataFrame from the sparse matrix `X_tfidf`.\n",
    "- Optionally, add the 'review' column to the DataFrame using `df['review'].values`.\n",
    "- Print the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8lKT03SmUIo",
    "outputId": "d848d9bf-84b2-4b67-b0e4-c5e56932c0c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF DataFrame:\n",
      "   aaron  abandon  abc  ability  able  aboard  abraham  abrupt  abruptly  \\\n",
      "0    0.0      0.0  0.0      0.0   0.0     0.0      0.0     0.0       0.0   \n",
      "1    0.0      0.0  0.0      0.0   0.0     0.0      0.0     0.0       0.0   \n",
      "2    0.0      0.0  0.0      0.0   0.0     0.0      0.0     0.0       0.0   \n",
      "3    0.0      0.0  0.0      0.0   0.0     0.0      0.0     0.0       0.0   \n",
      "4    0.0      0.0  0.0      0.0   0.0     0.0      0.0     0.0       0.0   \n",
      "\n",
      "   absence  ...     youll     young  youngster     youre  youth  youve  zero  \\\n",
      "0      0.0  ...  0.061217  0.000000        0.0  0.000000    0.0    0.0   0.0   \n",
      "1      0.0  ...  0.000000  0.000000        0.0  0.000000    0.0    0.0   0.0   \n",
      "2      0.0  ...  0.000000  0.076121        0.0  0.000000    0.0    0.0   0.0   \n",
      "3      0.0  ...  0.000000  0.000000        0.0  0.080674    0.0    0.0   0.0   \n",
      "4      0.0  ...  0.000000  0.000000        0.0  0.000000    0.0    0.0   0.0   \n",
      "\n",
      "     zombie  zone  zoom  \n",
      "0  0.000000   0.0   0.0  \n",
      "1  0.000000   0.0   0.0  \n",
      "2  0.000000   0.0   0.0  \n",
      "3  0.112757   0.0   0.0  \n",
      "4  0.000000   0.0   0.0  \n",
      "\n",
      "[5 rows x 5000 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert the sparse matrix to a DataFrame\n",
    "X_tfidf_df =pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Optional: Add the 'review' column to the DataFrame if needed\n",
    "#X_tfidf_df['review'] = df['review'].values\n",
    "\n",
    "# Print the DataFrame\n",
    "\n",
    "print(\"TF-IDF DataFrame:\")\n",
    "print(X_tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvsIdzyNhEfr"
   },
   "source": [
    "## T2.3: Split the dataset into train and test in the ratio of 80:20. (weightage – 5 marks) (ME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- Write a method using the `train_test_split` function from the `sklearn.model_selection` module to split data into training and testing sets.\n",
    "- Use the feature matrix `X_tfidf.toarray()` and target vector `y`. \n",
    "- Set the test size to 20% and use a random state of 42 for reproducibility. The method should return `X_train`, `X_test`, `y_train`, and `y_test` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "tZALLK9TYQEa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (40000, 5000)\n",
      "X_test shape: (10000, 5000)\n",
      "y_train shape: (40000,)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# split into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "def split_train_test(X, y, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Use the method to split the data\n",
    "X_train, X_test, y_train, y_test = split_train_test(X_tfidf.toarray(), y)\n",
    "\n",
    "# Print shapes to verify the splits\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBlNE53AhoLA"
   },
   "source": [
    "#### T2.4: Train a Sequence classifier using standard Machine learning algorithm(Logistic Regression)  to classify  documents as either positive or negative . (weightage - 5 marks) (ME)\n",
    "\n",
    "**Model versioning:**\n",
    "\n",
    "- Save the model as ‘first_model’ to a version control system GitHub using git commands for collaboration, tracking changes, and ensuring transparency in model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refer to the Github document from Lumen to create the repository and steps to commit \n",
    "#### Add your Github repository link below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- Define a logistic regression model for classification (`lr`) using the sklearn library.\n",
    "- Set regularization penalty as L2, maximum iterations as 500, regularization strength as 1, and random state as 42.\n",
    "- Fit the logistic regression model (`lr_classifier`) using training data (`X_train` and `y_train`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "nM3otGRUYQKW"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1, max_iter=500, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1, max_iter=500, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=1, max_iter=500, random_state=42)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "#training the model\n",
    "lr_classifier = LogisticRegression(penalty='l2', max_iter=500, C=1, random_state=42)\n",
    "\n",
    "#Fitting the model\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# saving model\n",
    "#with open('first_model.pkl', 'wb') as file:\n",
    "    #pickle.dump(lr_classifier, file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxT5f3tSiKjf"
   },
   "source": [
    "#### T2.5: Train Multilayer Perceptron (MLP) models to classify documents as either positive or negative. (weightage - 10 marks) (ME)\n",
    "\n",
    "**Model versioning**\n",
    "-Save the model as ‘second_model’ to a version control system GitHub using git commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- Import the Sequential and Dense modules from keras.models.\n",
    "- Define a Sequential model using `Sequential()`.\n",
    "- Add a dense layer with 16 units and ReLU activation using `model_mlp.add(Dense(...))`.\n",
    "- Add another dense layer with 8 units and ReLU activation.\n",
    "- Add a dense layer with 1 unit and sigmoid activation.\n",
    "- Compile the model with 'rmsprop' optimizer and binary crossentropy loss using `model_mlp.compile()`.\n",
    "- Train the model using `fit()`, specifying input data (`X_train`) and labels (`y_train`), batch size (10), and number of epochs (15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nK-mi7a6i2p-",
    "outputId": "3f72e9ea-8252-448b-d84e-2762f249efcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 16:34:34.649701: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-30 16:34:34.652390: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-30 16:34:34.697965: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-30 16:34:34.698279: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-30 16:34:35.424333: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "4000/4000 [==============================] - 6s 1ms/step - loss: 0.3193 - accuracy: 0.8690\n",
      "Epoch 2/15\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2547 - accuracy: 0.8968\n",
      "Epoch 3/15\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2462 - accuracy: 0.9013\n",
      "Epoch 4/15\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2420 - accuracy: 0.9043\n",
      "Epoch 5/15\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2394 - accuracy: 0.9051\n",
      "Epoch 6/15\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2377 - accuracy: 0.9071\n",
      "Epoch 7/15\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2344 - accuracy: 0.9086\n",
      "Epoch 8/15\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2323 - accuracy: 0.9094\n",
      "Epoch 9/15\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2296 - accuracy: 0.9105\n",
      "Epoch 10/15\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2267 - accuracy: 0.9122\n",
      "Epoch 11/15\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2233 - accuracy: 0.9145\n",
      "Epoch 12/15\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2209 - accuracy: 0.9158\n",
      "Epoch 13/15\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2169 - accuracy: 0.9193\n",
      "Epoch 14/15\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2131 - accuracy: 0.9215\n",
      "Epoch 15/15\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2072 - accuracy: 0.9246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f560cf5d490>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model_mlp = Sequential()\n",
    "\n",
    "#training the model\n",
    "# Add a dense layer with 16 units and ReLU activation\n",
    "model_mlp.add(Dense(16, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "# Add a dense layer with 8 units and ReLU activation\n",
    "model_mlp.add(Dense(8, activation='relu'))\n",
    "\n",
    "# Add a dense layer with 1 unit and sigmoid activation\n",
    "model_mlp.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with 'rmsprop' optimizer and binary crossentropy loss\n",
    "model_mlp.compile(optimizer=RMSprop(),loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Fitting the model\n",
    "model_mlp.fit(X_train, y_train, epochs=15, batch_size=10)\n",
    "\n",
    " #saving model\n",
    "#with open('second_model.pkl', 'wb') as file:\n",
    "    #pickle.dump(model_mlp,file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_igs5c35jznV"
   },
   "source": [
    "#### T2.6: Train a CNN with Embedding layer to classify documents as either positive or negative.  (weightage-5 marks)(ME)\n",
    "\n",
    "**Model versioning**\n",
    "-  Save the model as ‘third_model’ to a version control system GitHub using git commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- Create a CNN model for text classification.\n",
    "- Import required modules from Keras and TensorFlow.\n",
    "- Set the maximum number of words and sequence length.\n",
    "- Use Tokenizer to tokenize and convert text to integers.\n",
    "- Pad sequences to ensure they're all the same length.\n",
    "- Split the data into training and testing sets.\n",
    "    - Define the CNN model's architecture:\n",
    "        - Include an embedding layer for word embeddings.\n",
    "        - Add a Conv1D layer with 128 filters and a kernel size of 5.\n",
    "        - Include a GlobalMaxPooling1D layer to reduce dimensionality.\n",
    "        - Add two Dense layers with 64 and 1 units respectively.\n",
    "- Compile the model using the Adam optimizer and binary crossentropy loss.\n",
    "- Train the model for 10 epochs with a batch size of 32 and validate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mASSZLvMaauH",
    "outputId": "deedd4e3-9ee2-455a-b6b7-55f17804a8b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labuser/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py:1700: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 8s 7ms/step - loss: 0.3947 - accuracy: 0.8122 - val_loss: 0.3194 - val_accuracy: 0.8655\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2064 - accuracy: 0.9204 - val_loss: 0.3265 - val_accuracy: 0.8665\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0739 - accuracy: 0.9768 - val_loss: 0.4401 - val_accuracy: 0.8572\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0178 - accuracy: 0.9958 - val_loss: 0.5942 - val_accuracy: 0.8581\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 0.7348 - val_accuracy: 0.8591\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 4.0838e-04 - accuracy: 1.0000 - val_loss: 0.7448 - val_accuracy: 0.8644\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 7.7681e-05 - accuracy: 1.0000 - val_loss: 0.7872 - val_accuracy: 0.8646\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 3.7974e-05 - accuracy: 1.0000 - val_loss: 0.8258 - val_accuracy: 0.8635\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1711e-05 - accuracy: 1.0000 - val_loss: 0.8634 - val_accuracy: 0.8636\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.2629e-05 - accuracy: 1.0000 - val_loss: 0.9013 - val_accuracy: 0.8640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f55e4f2c460>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding,Conv1D,GlobalMaxPooling1D,Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Set the maximum number of words and sequence length\n",
    "max_words = 10000  # Maximum number of words to consider in the vocabulary\n",
    "max_len = 100      # Maximum sequence length\n",
    "\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['review'])\n",
    "sequences = tokenizer.texts_to_sequences(df['review'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Define target variable\n",
    "y = df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences,y,test_size=0.2,random_state=42)\n",
    "\n",
    "# Build the CNN model\n",
    "model_cnn = Sequential()\n",
    "# Add an embedding layer\n",
    "model_cnn.add(Embedding(input_dim=max_words, output_dim=50, input_length=max_len))\n",
    "# Add a Conv1D layer with 128 filters and kernel size of 5\n",
    "model_cnn.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "# Add a GlobalMaxPooling1D layer\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "# Add a Dense layer with 64 units and ReLU activation\n",
    "model_cnn.add(Dense(64, activation='relu'))\n",
    "# Add a Dense layer with 1 unit and sigmoid activation for binary classification\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model using the Adam optimizer and binary crossentropy loss\n",
    "model_cnn.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 10 epochs with a batch size of 32 and validate the results\n",
    "model_cnn.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "# Save the trained CNN model as 'third_model.h5'\n",
    "#with open('third_model.pkl','wb') as file:\n",
    "    #pickle.dump(model_cnn,file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLg5N2K3jBpx"
   },
   "source": [
    "# Task 3: Evaluate the performance of the model using the right evaluation metrics.                                                                         (weightage - 25 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJs_LaL9jGFQ"
   },
   "source": [
    "#### T3.1 Bring the models from a GitHub using git commands and evaluate the model (weightage - 2marks) (ME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "0hUpPhaEjRDm"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RMSprop' object has no attribute 'build'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m           lr_classifier\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 6\u001b[0m           model_mlp\u001b[38;5;241m=\u001b[39m\u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#with open('third_model.pkl','rb') as f:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m           \u001b[38;5;66;03m#model_cnn=pickle.load(f)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#Fitting the model\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/saving/pickle_utils.py:48\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[0;34m(serialized_model)\u001b[0m\n\u001b[1;32m     46\u001b[0m     model \u001b[38;5;241m=\u001b[39m saving_lib\u001b[38;5;241m.\u001b[39mload_model(filepath, safe_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/saving/pickle_utils.py:46\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[0;34m(serialized_model)\u001b[0m\n\u001b[1;32m     40\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(serialized_model)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# When loading, direct import will work for most custom objects\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# though it will require get_config() to be implemented.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Some custom objects (e.g. an activation in a Dense layer,\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# serialized as a string by Dense.get_config()) will require\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# a custom_object_scope.\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/saving/saving_lib.py:277\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    274\u001b[0m             asset_store\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/saving/saving_lib.py:242\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 242\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _VARS_FNAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_filenames:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/saving/serialization_lib.py:508\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m     compile_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compile_config:\n\u001b[0;32m--> 508\u001b[0m         \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompile_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared_object_id\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n\u001b[1;32m    511\u001b[0m     record_object_after_deserialization(\n\u001b[1;32m    512\u001b[0m         instance, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared_object_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    513\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py:3392\u001b[0m, in \u001b[0;36mModel.compile_from_config\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   3389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[1;32m   3390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[1;32m   3391\u001b[0m     \u001b[38;5;66;03m# Create optimizer variables.\u001b[39;00m\n\u001b[0;32m-> 3392\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/optimizers/legacy/optimizer_v2.py:984\u001b[0m, in \u001b[0;36mOptimizerV2.__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hyper:\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_hyper(name)\n\u001b[0;32m--> 984\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/optimizers/legacy/optimizer_v2.py:974\u001b[0m, in \u001b[0;36mOptimizerV2.__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Overridden to support hyperparameter access.\"\"\"\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# Needed to avoid infinite recursion with __setattr__.\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hyper\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RMSprop' object has no attribute 'build'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#training the model\n",
    "with open('first_model.pkl','rb') as f:\n",
    "          lr_classifier=pickle.load(f)\n",
    "with open('second_model.pkl','rb') as f:\n",
    "          model_mlp=pickle.load(f)\n",
    "#with open('third_model.pkl','rb') as f:\n",
    "          #model_cnn=pickle.load(f)\n",
    "\n",
    "#Fitting the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S8V89NQjRrZ"
   },
   "source": [
    "#### T3.2 Evaluate the Logistic Regression model with evaluation metrics accuracy and precision using sklearn library. (weightage-5 marks) (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "- __Function Definition:__ Define a function named evaluate_classification taking y_true (true labels) and X_test (test data).\n",
    "- __Prediction:__ Predict labels using a logistic regression classifier (lr_classifier) on test data (X_test) and save the result as y_pred.\n",
    "- __Evaluation Metrics:__\n",
    "    * Calculate accuracy using `accuracy_score` function.\n",
    "    * Calculate precision using `precision_score` function.\n",
    "    * Calculate recall using `recall_score` function.\n",
    "    * Calculate F1 score using `f1_score` function.\n",
    "- __Storage:__ Store all metrics in a dictionary named metrics.\n",
    "- __Return:__ Return the dictionary containing evaluation metrics.\n",
    " #### Ranges\n",
    "* Accuracy : 0.45 - 1 (2M)\n",
    "* Precision: 0.45-1 (1M) \n",
    "* Recall: 0.55-1 (1M)\n",
    "* F1 Score: 0.5 -1(1M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "os63a5fyjYAs"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precison_score,recall_score,f1_score\n",
    "#Predicting the model\n",
    "def evaluate_classification(lr_classifier,y_true, X_test):\n",
    "    accuracy,precision,recall,f1 = 0.0,0.0,0.0,0.0\n",
    "    y_pred=lr_classifier.predict(X_test)\n",
    "    precison_score(y_true,y_pred)\n",
    "    recall=recall_score(y_true,y_pred)\n",
    "    f1=f1_score(y_true,y_pred)\n",
    "    \n",
    "    return accuracy,precision,recall,f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Call the function with appropriate test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KZgmbu4ljdCT",
    "outputId": "7f6e23cc-4160-44db-cae7-d7e7cb5c18e2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# call evaluate_classification\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m evaluate_classification(lr_classifier,y_test, X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "# call evaluate_classification\n",
    "evaluate_classification(lr_classifier,y_test, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXqrmURrjn4T"
   },
   "source": [
    "#### T3.3 Using Lime/SHAP libraries, explain the prediction of your model and give inferences. (weightage-5 marks) (ME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vnqGUmpjwy5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uPQ9uZnjxFt"
   },
   "source": [
    "#### T3.4 For the trained MLP model used, specify the accuracy score, loss value, epochs and activation function used at the output layer of the model (weightage-8 marks)(AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Added model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model_mlp = Sequential()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "Define a method named `evaluate_mlp` to assess the performance of a __Multi-layer Perceptron (MLP) model__.\n",
    "Inside the method:\n",
    "\n",
    "- Utilize the model's evaluate function to compute the loss and accuracy using the test data (X_test and y_test).\n",
    "- Determine the number of epochs by calculating the length of the loss history.\n",
    "- Extract the name of the output activation function used in the last layer of the model.\n",
    "- Return the computed loss, accuracy and the name of the output activation function.\n",
    "\n",
    "Remember to:\n",
    "- Input the trained MLP model, test data (X_test and y_test), and training history.\n",
    "- Use the method as follows: loss, accuracy, output_activation_function = evaluate_mlp(model, X_test, y_test, history)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x45_PcQGj26T"
   },
   "outputs": [],
   "source": [
    "#Evaluate the Model\n",
    "def evaluate_mlp(model,X_test,y_test,history):\n",
    "    accuracy,loss ,output_activation_function = 0.0,0.0,None\n",
    "\n",
    "    return accuracy,loss ,output_activation_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use method evaluate_mlp to assess the MLP model's performance, then print accuracy, loss, epochs, and output activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-VGfx3lklFP",
    "outputId": "798229b6-c47f-4ac3-d73e-66a74930b049"
   },
   "outputs": [],
   "source": [
    "# Print loss, accuracy,output_activation_function \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jAlgnNHkxYi"
   },
   "source": [
    "#### T3.5 For the Trained CNN with Embedding layer, Specify the accuracy score, loss value, epochs used .(weightage-5marks) (ME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    " \n",
    "__Tokenize and pad sequences:__\n",
    "- Define the maximum number of words as 10,000 and the maximum length of sequences as 100.\n",
    "- Initialize a tokenizer.\n",
    "- Teach the tokenizer about the data with **`fit_on_texts(X)`**.\n",
    "- Convert the texts to sequences using **`texts_to_sequences(X)`**.\n",
    "- Pad the sequences to ensure they're all the same length using **`pad_sequences()`**.\n",
    " \n",
    "__Split the Data:__\n",
    "- Divide the data into training and testing sets with 80% for training and 20% for testing.\n",
    "- Utilize **`train_test_split()`** with the defined parameters.\n",
    " \n",
    "__Build the CNN Model:__\n",
    "- Set the embedding dimension as 50 and the vocabulary size as the maximum words.\n",
    "- Create a Sequential model.\n",
    "- Add an Embedding layer with the specified parameters.\n",
    "- Include a 1D Convolutional layer with 128 filters, a kernel size of 5, and ReLU activation.\n",
    "- Apply GlobalMaxPooling1D to reduce the dimensionality.\n",
    "- Integrate two Dense layers with 64 and 1 neuron(s), respectively, using ReLU and sigmoid activations.\n",
    " \n",
    "__Compile the Model:__\n",
    "- Compile the model with the Adam optimizer and binary cross-entropy loss.\n",
    "- Specify **'accuracy'** as the metric for evaluation.\n",
    " \n",
    "__Train the Model:__\n",
    "- Train the model on the training data for 10 epochs with a batch size of 32.\n",
    "- Validate the model with 10% of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "\n",
    "\n",
    "# Split the Data\n",
    "\n",
    "\n",
    "# Build the CNN Model\n",
    "\n",
    "\n",
    "# Compile the Model\n",
    "\n",
    "\n",
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "Create a method named `evaluate_cnn` to assess the performance of a __Convolutional Neural Network (CNN) model.__\n",
    "- Within the method:\n",
    "    - Utilize the model's evaluate function to calculate the loss and accuracy using the test data (X_test and y_test).\n",
    "    - Determine the number of epochs by extracting the length of the loss history from the training history.\n",
    "    - Return the computed loss, accuracy, and number of epochs.\n",
    "\n",
    "Remember to:\n",
    "- Provide the trained CNN model, test data (X_test and y_test), and training history as input parameters.\n",
    "- Utilize the method like this: loss, accuracy, epochs = evaluate_cnn(model, X_test, y_test, history)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUMVjwbck81v"
   },
   "outputs": [],
   "source": [
    "#Evaluate the Model\n",
    "def evaluate_cnn(model,X_test,y_test,history):\n",
    "    loss, accuracy,epochs = 0.0,0.0,0.0\n",
    "    \n",
    "    return loss, accuracy,epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Invoke the evaluate_mlp method with arguments (model_cnn, X_test, y_test, history_cnn).\n",
    "- Print the accuracy score, loss value and the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R-5NWRqQmFtb",
    "outputId": "52daef53-30be-4900-c907-50a3785b27f0"
   },
   "outputs": [],
   "source": [
    "# Print accuracy,loss,epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcH6vRlvmSUc"
   },
   "source": [
    "#### T3.6 Implement the unit test case and deploy a model using Flask / Streamlit. (weightage-10 marks)(ME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "- Import the necessary libraries: __keras.models__ for Sequential model and __keras.layers__ for Dense layers.\n",
    "- Create a new Sequential model named __keras_model.__\n",
    "- Add layers to the Sequential model and define their configurations (units, activation functions, input dimensions).\n",
    "- Set the weights of the layers in the new model to be the same as the weights of an existing model (model).\n",
    "- Save the Keras model to an HDF5 file named _'final_model.h5'.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4YAKslcEsgnc",
    "outputId": "dfd79646-df8c-450b-9fbe-9248d81e9c83"
   },
   "outputs": [],
   "source": [
    "# Create a new Sequential model\n",
    "\n",
    "\n",
    "# Add layers to the Sequential model and set the weights\n",
    "\n",
    "\n",
    "# Set the weights of the layers in the new model\n",
    "\n",
    "\n",
    "# Save the Keras model to an HDF5 file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the necessary module json for working with JSON data.\n",
    "- Convert the tokenizer's configuration to JSON format using the __to_json()__ method.\n",
    "- Open a file named __'imdb_tokenizer.json'__ in write mode and encode it in UTF-8.\n",
    "- Write the JSON data into the file using __json.dumps()__ function, ensuring non-ASCII characters are handled properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer configuration to JSON file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Summarize the findings of the analysis and draw conclusions with PPT / PDF.                                                                                   (weightage - 15 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Submission guidelines:** \n",
    "1.\tDownload the Jupyter notebook in the format of html. \n",
    "2.\tUpload it in the lumen (UNext LMS)\n",
    "3.\tTake a screenshot of T3.6(Deployment) and upload it in the lumen. (UNext LMS)\n",
    "4.\tSummarized PPT/ PDF prepared in Task 4 to be uploaded in the lumen. (UNext LMS)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
